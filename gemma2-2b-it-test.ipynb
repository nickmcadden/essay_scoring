{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8068185,"sourceType":"datasetVersion","datasetId":4760283},{"sourceId":164418066,"sourceType":"kernelVersion"},{"sourceId":85995,"sourceType":"modelInstanceVersion","modelInstanceId":72254,"modelId":76277}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"VER=1\n# USE NEXT K ROWS FOR TRAIN\nTRAIN_SET = 2500\n# USE FIRST K ROWS FOR VALIDATION\nVAL_SET = 100\n# MAX SEQ LENGTH FOR MISTRAL TRAINING\nMAX_LEN = 1024\n# BATCH SIZE PER DEVICE\nBATCH_SIZE = 1\n# NUMBER OF EPOCHS\nEPOCHS = 1\n# IF LOAD PATH IS NOT NONE, LOAD A PREVIOUSLY TRAINED MODEL\nLOAD_PATH = None     #'/kaggle/input/mistral-v0/'\n\nimport pandas as pd, numpy as np\nfrom sklearn.metrics import cohen_kappa_score\ndf = pd.read_csv('/kaggle/input/learning-agency-lab-automated-essay-scoring-4/train.csv')\nprint('Train shape:', df.shape )\nprint(df.head())\n\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Necessary until transformers packages is updated in the Kaggle notebook environment.\n\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\nimport torch\nimport re\n\n#model_name = 'mistralai/Mistral-7B-Instruct-v0.2' # WHEN INTERNET IS TURNED ON\n#model_name = '/kaggle/input/mistral-7b-instruct-v02-fp16'\n#model_name = \"/kaggle/input/gemma-2/transformers/gemma-2-2b-it/1/\"\nmodel_name = \"/kaggle/input/gemma-2/transformers/gemma-2-9b-it/2/\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nbase_model = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=torch.bfloat16,\n    device_map=\"auto\",\n    attn_implementation='eager'\n)\n\ntokenizer.pad_token = tokenizer.eos_token\ninput_text = \"Write me a poem about Machine Learning which is 4 lines long.\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").to('cuda')\n\noutputs = base_model.generate(**input_ids, max_new_tokens=256)\nprint(tokenizer.decode(outputs[0][len(input_ids[0]):], skip_special_tokens=True))\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls /kaggle/input/gemma-2/transformers/gemma-2-9b-it/2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!cd /opt/conda/lib/python3.10/site-packages/transformers/\n!cat /opt/conda/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocess(sample, text=False, infer_mode=False, max_seq=MAX_LEN, return_tensors=None):\n\n    sys_prompt = \"Mark this year 10 essay and assign a score of 1,2,3,4,5,6 where 6 is the best. Output only a single number with no explanation:\"\n    prompt = sample[\"full_text\"]\n    \n    messages = [{\"role\": \"user\", \"content\": sys_prompt + prompt}]\n    \n    if not infer_mode:\n        messages.append({\"role\": \"assistant\", \"content\": str(sample[\"score\"])})\n\n    formatted_sample = tokenizer.apply_chat_template(messages, tokenize=False)\n    if infer_mode:\n        formatted_sample = formatted_sample.replace(\"</s>\",\"\")\n        #print(formatted_sample)\n    \n    tokenized_sample = tokenizer(formatted_sample, padding=True, return_tensors=return_tensors, \n                                 truncation=True, add_special_tokens=False, max_length=max_seq) \n    \n    if return_tensors==\"pt\":\n        tokenized_sample[\"labels\"] = tokenized_sample[\"input_ids\"].clone()\n    else:\n        tokenized_sample[\"labels\"] = tokenized_sample[\"input_ids\"].copy()\n    \n    if text: return formatted_sample\n    else: return tokenized_sample\n\n\ndef evaluate_model(df, model):\n    preds = []\n\n    for i,row in df.iloc[:VAL_SET].iterrows():\n        tokenized_sample = preprocess(row, infer_mode=True, max_seq=2048, return_tensors=\"pt\")\n        generated_ids = model.generate(**tokenized_sample, \n                                        max_new_tokens=5,\n                                        pad_token_id=tokenizer.eos_token_id,\n                                        do_sample=False)\n        decoded = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n        \n        try:\n            #answer = decoded[0].rsplit(\"The score is: \", 1)[1] # Mistral\n            score = tokenizer.decode(generated_ids[0][len(tokenized_sample[0]):], skip_special_tokens=True) # Gemma2\n            score = int(re.search(r'\\d+', score).group())\n            score_output = score\n        except:\n            score_output = score\n            score = 3\n        \n        preds.append(int(score))\n        \n        print(i, 'Predicted:', score_output, ' Actual:', row['score'])\n        print(f'predicted scores={preds}, \\n',end='')\n    evaluation_score = cohen_kappa_score(df.score.values[:VAL_SET], preds, weights=\"quadratic\")\n\n    return evaluation_score\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datasets import Dataset\n\ndataset_v = Dataset.from_pandas(df.iloc[:VAL_SET])\ndataset_t = Dataset.from_pandas(df.iloc[VAL_SET:VAL_SET+TRAIN_SET])\n\ntokenized_dataset_v = dataset_v.map(preprocess, num_proc=4, \n                            remove_columns=['essay_id', 'full_text', 'score'])\ntokenized_dataset_t = dataset_t.map(preprocess, num_proc=4, \n                            remove_columns=['essay_id', 'full_text', 'score'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Score the model vs the validation set before training\nprint(evaluate_model(df, base_model))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install peft==0.10.0","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import Trainer, TrainingArguments, DataCollatorForSeq2Seq\nfrom peft import LoraConfig, get_peft_model, PeftModel\n\ntraining_args = TrainingArguments(\n    per_device_train_batch_size=BATCH_SIZE,\n    num_train_epochs=EPOCHS,\n    logging_dir = f'./logs_v{VER}',\n    output_dir = f'./output_v{VER}',\n    logging_steps=25,\n    save_strategy='no',\n    load_best_model_at_end=True,\n    logging_first_step=True,\n    overwrite_output_dir=True,\n    warmup_ratio=0.0,\n    learning_rate=5e-4,\n    lr_scheduler_type='constant',\n    weight_decay=0.01,\n    eval_steps=None,\n    evaluation_strategy='no',\n    report_to='none',\n)\n\npeft_config = LoraConfig(\n    lora_alpha=16, # regularization\n    lora_dropout=0.1, \n    r=32, # attention heads\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\"], \n)\n\nif not LOAD_PATH:\n    adapter_model = get_peft_model(base_model, peft_config)\n\ntrainer = Trainer(\n    model=adapter_model,\n    args=training_args,\n    train_dataset=tokenized_dataset_t,\n    eval_dataset=tokenized_dataset_v,\n    data_collator=DataCollatorForSeq2Seq(tokenizer, padding='longest'),\n)\n\nif not LOAD_PATH:\n    trainer.train()\n    trainer.model.save_pretrained(\"gemma2-2b-it-adapter\")\nelse:\n    model = PeftModel.from_pretrained(model, LOAD_PATH)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trained_model = trainer.model.merge_and_unload()\n# Score the new trained model vs the validation set to see if the training has worked\nprint(evaluate_model(df, trained_model))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#!pip install peft==0.11.0\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, DataCollatorForSeq2Seq\nfrom peft import LoraConfig, get_peft_model, PeftModel\nimport torch\n\nmodel_name = \"/kaggle/input/gemma-2/transformers/gemma-2-2b-it/1/\"\n\ntraining_args = TrainingArguments(\n    per_device_train_batch_size=1,\n    num_train_epochs=1,\n    logging_dir = f'.',\n    output_dir = f'.',\n    logging_steps=25,\n    save_strategy='epoch',\n    load_best_model_at_end=True,\n    logging_first_step=True,\n    overwrite_output_dir=True,\n    warmup_ratio=0.0,\n    learning_rate=5e-4,\n    lr_scheduler_type='constant',\n    weight_decay=0.01,\n    eval_steps=None,\n    evaluation_strategy='epoch',\n    report_to='none',\n)\n\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nbase_model = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=torch.bfloat16,\n    device_map=\"auto\",\n    attn_implementation='eager'\n)\n\nprint('base_model', base_model.num_parameters())\nbase_model.save_pretrained(\"gemma2-2b-it\")\n\npeft_config = LoraConfig(\n    lora_alpha=16, # regularization\n    lora_dropout=0.1, \n    r=32, # attention heads\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\"], \n)\n\n# create and save an adapter\nadapter_model = get_peft_model(base_model, peft_config)\nadapter_path = \"gemma2-2b-it-adapter\"\nadapter_model.save_pretrained(adapter_path)\nprint('adapter', adapter_model.num_parameters())\n\n# create a peft model from a gemma model and adapter\n#model = PeftModel.from_pretrained(base_model, adapter_path, peft_config = peft_config)\n#print('PeftModel', model.num_parameters())\n\ntrainer = Trainer(\n    model=adapter_model,\n    args=training_args,\n    data_collator=DataCollatorForSeq2Seq(tokenizer, padding='longest'),\n)\n\nprint('TrainerModel', trainer.model.num_parameters())\n\ntrainer.model.merge_and_unload()\n\nprint('TrainerMergedModel', trainer.model.num_parameters())\n\ntrainer.model.save_pretrained('gemma2-2b-it-trained')\n\nmodel.save_pretrained(\"gemma2-2b-it-adaptor\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls -l /kaggle/working/gemma2-2b-it-adapter","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}